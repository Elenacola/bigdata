{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi del prezzo delle Azioni di Apple\n",
    "In questo notebook faremo un po' di pratica con Spark, Dataframe e serie storiche (timeseries). Come lo faremo ? Andando ad analizzare il titolo azionario di Apple (AAPL), nello specifico cercheremo risposta per le seguenti domande: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Qual è stato il valore massimo raggiunto dal AAPL ? In che data lo ha raggiunto ?\n",
    "* Qual è stato il valore minimo raggiunto dal AAPL dopo il 2000 ? In che data lo ha raggiunto ?\n",
    "* Qual è la percentuale di giorni in cui il prezzo di chiusura è stato inferiore ai 100 USD ?\n",
    "* Qual è la percentuale di giorni dopo il 2014 in cui il prezzo di chiusura è stato inferiore ai 100 USD ? \n",
    "* Visualizza il valore massimo per anno.\n",
    "* In quale anno sono state scambiate più azioni di Apple (anno con il volume maggiore).\n",
    "* Il 29 Giugno 2007 è stato rilasciato al pubblico il primo iPhone, come è variato il prezzo delle azioni di Apple nei 180 giorni successivi ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procuriamoci il dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inizializziamo Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('basic').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importiamo il Dataset all'interno di un Dataframe\n",
    "Per importare il file csv possiamo utilizzare il solito metodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------+--------+--------+---------+---------+\n",
      "|               Date|    Open|    High|     Low|   Close|Adj Close|   Volume|\n",
      "+-------------------+--------+--------+--------+--------+---------+---------+\n",
      "|1980-12-12 00:00:00|0.513393|0.515625|0.513393|0.513393| 0.410525|117258400|\n",
      "|1980-12-15 00:00:00|0.488839|0.488839|0.486607|0.486607| 0.389106| 43971200|\n",
      "|1980-12-16 00:00:00|0.453125|0.453125|0.450893|0.450893| 0.360548| 26432000|\n",
      "|1980-12-17 00:00:00|0.462054|0.464286|0.462054|0.462054| 0.369472| 21610400|\n",
      "|1980-12-18 00:00:00|0.475446|0.477679|0.475446|0.475446| 0.380182| 18362400|\n",
      "|1980-12-19 00:00:00|0.504464|0.506696|0.504464|0.504464| 0.403385| 12157600|\n",
      "|1980-12-22 00:00:00|0.529018|0.531250|0.529018|0.529018| 0.423019|  9340800|\n",
      "|1980-12-23 00:00:00|0.551339|0.553571|0.551339|0.551339| 0.440868| 11737600|\n",
      "|1980-12-24 00:00:00|0.580357|0.582589|0.580357|0.580357| 0.464072| 12000800|\n",
      "|1980-12-26 00:00:00|0.633929|0.636161|0.633929|0.633929| 0.506909| 13893600|\n",
      "|1980-12-29 00:00:00|0.642857|0.645089|0.642857|0.642857| 0.514049| 23290400|\n",
      "|1980-12-30 00:00:00|0.629464|0.629464|0.627232|0.627232| 0.501554| 17220000|\n",
      "|1980-12-31 00:00:00|0.611607|0.611607|0.609375|0.609375| 0.487276|  8937600|\n",
      "|1981-01-02 00:00:00|0.616071|0.620536|0.616071|0.616071| 0.492630|  5415200|\n",
      "|1981-01-05 00:00:00|0.604911|0.604911|0.602679|0.602679| 0.481921|  8932000|\n",
      "|1981-01-06 00:00:00|0.578125|0.578125|0.575893|0.575893| 0.460502| 11289600|\n",
      "|1981-01-07 00:00:00|0.553571|0.553571|0.551339|0.551339| 0.440868| 13921600|\n",
      "|1981-01-08 00:00:00|0.542411|0.542411|0.540179|0.540179| 0.431944|  9956800|\n",
      "|1981-01-09 00:00:00|0.569196|0.571429|0.569196|0.569196| 0.455147|  5376000|\n",
      "|1981-01-12 00:00:00|0.569196|0.569196|0.564732|0.564732| 0.451577|  5924800|\n",
      "+-------------------+--------+--------+--------+--------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"AAPL.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stampiamo lo schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo schema è stato interpretato male, tutte le colonne, ad eccezione del timestamp, sono state lette come stringhe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correggiamo lo schema\n",
    "Definiamo manualmente lo schema con i tipi corretti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------+--------+--------+---------+---------+\n",
      "|               Date|    Open|    High|     Low|   Close|Adj Close|   Volume|\n",
      "+-------------------+--------+--------+--------+--------+---------+---------+\n",
      "|1980-12-12 00:00:00|0.513393|0.515625|0.513393|0.513393| 0.410525|117258400|\n",
      "|1980-12-15 00:00:00|0.488839|0.488839|0.486607|0.486607| 0.389106| 43971200|\n",
      "|1980-12-16 00:00:00|0.453125|0.453125|0.450893|0.450893| 0.360548| 26432000|\n",
      "|1980-12-17 00:00:00|0.462054|0.464286|0.462054|0.462054| 0.369472| 21610400|\n",
      "|1980-12-18 00:00:00|0.475446|0.477679|0.475446|0.475446| 0.380182| 18362400|\n",
      "|1980-12-19 00:00:00|0.504464|0.506696|0.504464|0.504464| 0.403385| 12157600|\n",
      "|1980-12-22 00:00:00|0.529018| 0.53125|0.529018|0.529018| 0.423019|  9340800|\n",
      "|1980-12-23 00:00:00|0.551339|0.553571|0.551339|0.551339| 0.440868| 11737600|\n",
      "|1980-12-24 00:00:00|0.580357|0.582589|0.580357|0.580357| 0.464072| 12000800|\n",
      "|1980-12-26 00:00:00|0.633929|0.636161|0.633929|0.633929| 0.506909| 13893600|\n",
      "|1980-12-29 00:00:00|0.642857|0.645089|0.642857|0.642857| 0.514049| 23290400|\n",
      "|1980-12-30 00:00:00|0.629464|0.629464|0.627232|0.627232| 0.501554| 17220000|\n",
      "|1980-12-31 00:00:00|0.611607|0.611607|0.609375|0.609375| 0.487276|  8937600|\n",
      "|1981-01-02 00:00:00|0.616071|0.620536|0.616071|0.616071|  0.49263|  5415200|\n",
      "|1981-01-05 00:00:00|0.604911|0.604911|0.602679|0.602679| 0.481921|  8932000|\n",
      "|1981-01-06 00:00:00|0.578125|0.578125|0.575893|0.575893| 0.460502| 11289600|\n",
      "|1981-01-07 00:00:00|0.553571|0.553571|0.551339|0.551339| 0.440868| 13921600|\n",
      "|1981-01-08 00:00:00|0.542411|0.542411|0.540179|0.540179| 0.431944|  9956800|\n",
      "|1981-01-09 00:00:00|0.569196|0.571429|0.569196|0.569196| 0.455147|  5376000|\n",
      "|1981-01-12 00:00:00|0.569196|0.569196|0.564732|0.564732| 0.451577|  5924800|\n",
      "+-------------------+--------+--------+--------+--------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data_schema = [ #StructField('Date', DateType(), True),\n",
    "                StructField('Date', TimestampType(), True),\n",
    "                StructField('Open', FloatType(), True),\n",
    "                StructField('High', FloatType(), True),\n",
    "                StructField('Low', FloatType(), True),\n",
    "                StructField('Close', FloatType(), True),\n",
    "                StructField('Adj Close', FloatType(), True),\n",
    "                StructField('Volume', IntegerType(), True),]\n",
    "            \n",
    "schema = StructType(fields=data_schema)\n",
    "\n",
    "df = spark.read.schema(schema).option(\"header\",\"true\").option(\"inferSchema\",\"false\").csv(\"AAPL.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: float (nullable = true)\n",
      " |-- High: float (nullable = true)\n",
      " |-- Low: float (nullable = true)\n",
      " |-- Close: float (nullable = true)\n",
      " |-- Adj Close: float (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso lo schema è corretto, l'ora presente nel timestamp è un'informazione superflua, dato che coincide sempre con la mezzanotte, rimuoviamola."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertiamo il timestamp in una data\n",
    "Per rimuovere l'ora ci basta convertire il timestamp in una data, per farlo utilizziamo la funzione *.to_date(column, format)* di spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+--------+--------+---------+---------+\n",
      "|      Date|    Open|    High|     Low|   Close|Adj Close|   Volume|\n",
      "+----------+--------+--------+--------+--------+---------+---------+\n",
      "|1980-12-12|0.513393|0.515625|0.513393|0.513393| 0.410525|117258400|\n",
      "|1980-12-15|0.488839|0.488839|0.486607|0.486607| 0.389106| 43971200|\n",
      "|1980-12-16|0.453125|0.453125|0.450893|0.450893| 0.360548| 26432000|\n",
      "|1980-12-17|0.462054|0.464286|0.462054|0.462054| 0.369472| 21610400|\n",
      "|1980-12-18|0.475446|0.477679|0.475446|0.475446| 0.380182| 18362400|\n",
      "|1980-12-19|0.504464|0.506696|0.504464|0.504464| 0.403385| 12157600|\n",
      "|1980-12-22|0.529018| 0.53125|0.529018|0.529018| 0.423019|  9340800|\n",
      "|1980-12-23|0.551339|0.553571|0.551339|0.551339| 0.440868| 11737600|\n",
      "|1980-12-24|0.580357|0.582589|0.580357|0.580357| 0.464072| 12000800|\n",
      "|1980-12-26|0.633929|0.636161|0.633929|0.633929| 0.506909| 13893600|\n",
      "|1980-12-29|0.642857|0.645089|0.642857|0.642857| 0.514049| 23290400|\n",
      "|1980-12-30|0.629464|0.629464|0.627232|0.627232| 0.501554| 17220000|\n",
      "|1980-12-31|0.611607|0.611607|0.609375|0.609375| 0.487276|  8937600|\n",
      "|1981-01-02|0.616071|0.620536|0.616071|0.616071|  0.49263|  5415200|\n",
      "|1981-01-05|0.604911|0.604911|0.602679|0.602679| 0.481921|  8932000|\n",
      "|1981-01-06|0.578125|0.578125|0.575893|0.575893| 0.460502| 11289600|\n",
      "|1981-01-07|0.553571|0.553571|0.551339|0.551339| 0.440868| 13921600|\n",
      "|1981-01-08|0.542411|0.542411|0.540179|0.540179| 0.431944|  9956800|\n",
      "|1981-01-09|0.569196|0.571429|0.569196|0.569196| 0.455147|  5376000|\n",
      "|1981-01-12|0.569196|0.569196|0.564732|0.564732| 0.451577|  5924800|\n",
      "+----------+--------+--------+--------+--------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df = df.withColumn('Date', to_date(df[\"Date\"], \"yyyy-MM-dd\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diamo uno sguardo allo schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: float (nullable = true)\n",
      " |-- High: float (nullable = true)\n",
      " |-- Low: float (nullable = true)\n",
      " |-- Close: float (nullable = true)\n",
      " |-- Adj Close: float (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso il timestamp è diventato un tipo date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cerchiamo il giorno con il valore maggiore\n",
    "Per trovare il giorno in cui il prezzo del titolo ha raggiunto il valore maggiore dobbiamo semplicemente ordinare il dataset in base alla colonna *High*, in ordine discendente, poi stampiamo la prima riga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+---------+--------+\n",
      "|      Date|  Open|  High|   Low| Close|Adj Close|  Volume|\n",
      "+----------+------+------+------+------+---------+--------+\n",
      "|2018-10-03|230.05|233.47|229.78|232.07|229.39209|28654800|\n",
      "+----------+------+------+------+------+---------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "df.orderBy(\"High\", ascending=False).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cerchiamo il giorno con il valore minore dopo il 2000\n",
    "Anche in questo caso l'operazione è semplice, filtriamo i giorni successivi al primo Gennaio del 2000 e ordiniamoli in base alla colonna *Low*, in ordine ascendente, quindi stampiamo la prima riga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+--------+--------+---------+---------+\n",
      "|      Date|    Open|    High|     Low|   Close|Adj Close|   Volume|\n",
      "+----------+--------+--------+--------+--------+---------+---------+\n",
      "|2003-04-17|0.942857|0.946429|0.908571|0.937143| 0.820964|154064400|\n",
      "+----------+--------+--------+--------+--------+---------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Date > '2000-01-01'\").orderBy(\"Low\", ascending=True).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcoliamo la percentuale di giorni con un valore in chiusura minore di 100 USD \n",
    "Per calcolare la percentuale dobbiamo dividere il numero di giorni in cui il valore ha chiuso sotto i 100$ per il numero di gironi totali e moltiplicare per 100, dato che il numero avrà molte cifre dopo la virogola lo arrotondiamo utilizzando la funzio round di python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.67"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round((df.filter(df[\"Close\"]<100).count() / df.count())*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I giorni sono quasi il 90% ed è comune dato che stiamo considerando fin da quando la Apple è stata quotata in borsa negli anni 80."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contiamo i giorni successivi al 2014 con un valore in chiusura minore di 100 USD \n",
    "Ripetiamo lo stesso processo di sopra, ma questa volta filtriamo soltanto i giorno successivi al primo Gennaio 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+---------+---------+---------+\n",
      "|      Date|     Open|     High|      Low|    Close|Adj Close|   Volume|\n",
      "+----------+---------+---------+---------+---------+---------+---------+\n",
      "|2014-01-02| 79.38286|79.575714|    78.86| 79.01857| 71.59167| 58671200|\n",
      "|2014-01-03|    78.98|     79.1|77.204285| 77.28286|  70.0191| 98116900|\n",
      "|2014-01-06| 76.77857| 78.11429| 76.22857|77.704285|  70.4009|103152700|\n",
      "|2014-01-07|    77.76|77.994286| 76.84571|77.148575| 69.89742| 79302300|\n",
      "|2014-01-08|76.972855| 77.93714| 76.95571|77.637146|70.340096| 64632400|\n",
      "|2014-01-09| 78.11429|78.122856| 76.47857| 76.64571| 69.44184| 69787200|\n",
      "|2014-01-10| 77.11857| 77.25714|75.872856|76.134285| 68.97846| 76244000|\n",
      "|2014-01-13| 75.70143|     77.5| 75.69714| 76.53286|69.339584| 94623200|\n",
      "|2014-01-14| 76.88857| 78.10429| 76.80857| 78.05572| 70.71932| 83140400|\n",
      "|2014-01-15| 79.07429| 80.02857| 78.80857|79.622856| 72.13917| 97909700|\n",
      "|2014-01-16| 79.27143|    79.55| 78.81143| 79.17857| 71.73665| 57319500|\n",
      "|2014-01-17| 78.78286| 78.86714| 77.12857| 77.23857| 69.97897|106684900|\n",
      "|2014-01-21| 77.28429| 78.58143| 77.20286| 78.43857|71.066185| 82131700|\n",
      "|2014-01-22| 78.70143|79.612854| 78.25857| 78.78714|   71.382| 94996300|\n",
      "|2014-01-23| 78.56286|     79.5|    77.83|79.454285| 71.98645|100809800|\n",
      "|2014-01-24| 79.14286| 79.37428| 77.82143|    78.01|70.677895|107338700|\n",
      "|2014-01-27| 78.58143| 79.25714| 77.96429| 78.64286| 71.25127|138719700|\n",
      "|2014-01-28|    72.68| 73.57143| 71.72429| 72.35714| 65.55634|266380800|\n",
      "|2014-01-29| 71.99286| 72.48143| 71.23143| 71.53571|64.812126|125702500|\n",
      "|2014-01-30| 71.79143| 72.35714|70.957146| 71.39714|64.686554|169625400|\n",
      "+----------+---------+---------+---------+---------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2010 = df.filter(\"Date >= '2014-01-01'\")\n",
    "df2010.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.54"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round((df2010.filter(df[\"Close\"]<100).count() / df2010.count())*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso la Apple ha chiuso sotto i 100$ per soltanto il 21.5% del tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizziamo il massimo per anno\n",
    "Raggruppiamo il dataframe per anno, per estrarre solo l'anno dalla data possiamo usare la funzione *year(col)* di spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "dfGroupYear = df.groupBy(year(\"Date\").alias(\"Year\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora aggreghiamo in base al valore bassimo della colonna *High* e ordiniamo in base all'anno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Year|max(High)|\n",
      "+----+---------+\n",
      "|2018|   233.47|\n",
      "|2017|    177.2|\n",
      "|2016|   118.69|\n",
      "|2015|   134.54|\n",
      "|2014|   119.75|\n",
      "|2013| 82.16286|\n",
      "|2012|100.72429|\n",
      "|2011| 60.95714|\n",
      "|2010|46.665714|\n",
      "|2009|30.564285|\n",
      "|2008|28.608572|\n",
      "|2007|28.994286|\n",
      "|2006|13.308572|\n",
      "|2005|    10.78|\n",
      "|2004| 4.969285|\n",
      "|2003| 1.786429|\n",
      "|2002| 1.869286|\n",
      "|2001| 1.937143|\n",
      "|2000| 5.370536|\n",
      "|1999| 4.214286|\n",
      "|1998|   1.5625|\n",
      "|1997| 1.055804|\n",
      "|1996| 1.267857|\n",
      "|1995| 1.790179|\n",
      "|1994|   1.5625|\n",
      "|1993| 2.330357|\n",
      "|1992|      2.5|\n",
      "|1991| 2.616071|\n",
      "|1990| 1.705357|\n",
      "|1989| 1.799107|\n",
      "|1988| 1.705357|\n",
      "|1987| 2.133929|\n",
      "|1986| 0.783482|\n",
      "|1985| 0.555804|\n",
      "|1984| 0.613839|\n",
      "|1983| 1.129464|\n",
      "|1982| 0.622768|\n",
      "|1981| 0.620536|\n",
      "|1980| 0.645089|\n",
      "+----+---------+\n",
      "only showing top 39 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "dfHigh = dfGroupYear.agg(max(\"High\"))\n",
    "dfHigh.orderBy(\"Year\", ascending=False).show(39)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troviamo l'anno con il volume maggiore\n",
    "Utiliziamo il Dataframe raggruppato per anno che abbiamo creato sopra e sommiamo i volumi di ogni anno utilizzando il metodo *.agg*, poi stampiamo i 10 risultati ordinati per volumi totali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|Year|     Volume|\n",
      "+----+-----------+\n",
      "|2008|71495301500|\n",
      "|2007|61748996400|\n",
      "|2006|53924741500|\n",
      "|2005|45600245600|\n",
      "|2010|37756231800|\n",
      "|2009|35813421700|\n",
      "|1999|34275676400|\n",
      "|2012|32991051100|\n",
      "|2011|31014834900|\n",
      "|2004|30450417200|\n",
      "+----+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "dfVolume = dfGroupYear.agg(sum(\"Volume\").alias(\"Volume\"))\n",
    "dfVolume.orderBy(\"Volume\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifichiamo la variazione del titolo di Apple in seguito alla commercializzazione dell'iPhone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dobbiamo considerare il valore del titolo tra il 2007-06-29 e 180 giorni dopo, per calcolare la seconda data utilizziamo il modulo datetime di python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007-12-26\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_date = '2007-06-29'\n",
    "end_date = datetime.strptime(start_date, \"%Y-%m-%d\")+timedelta(days=180) # convertiamo la stringa in data e aggiungiamo 180 giorni\n",
    "end_date = datetime.strftime(end_date, \"%Y-%m-%d\") # convertiamo la data in stringa\n",
    "\n",
    "print(end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ottimo, ora estraiamo i valori per la data di inizio all'interno di un dizionario python, possiamo farlo usando un semplice filtro e poi il metodo *.asDict()* sull'oggetto Row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': datetime.date(2007, 6, 29),\n",
       " 'Open': 17.424285888671875,\n",
       " 'High': 17.714284896850586,\n",
       " 'Low': 17.29857063293457,\n",
       " 'Close': 17.43428611755371,\n",
       " 'Adj Close': 15.272941589355469,\n",
       " 'Volume': 284460400}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowFirst = df.filter(\"Date == '\"+start_date+\"'\").take(1)\n",
    "dictFirst = rowFirst[0].asDict()\n",
    "\n",
    "dictFirst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ripetiamo lo stesso procedimento per la data di fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': datetime.date(2007, 12, 26),\n",
       " 'Open': 28.43000030517578,\n",
       " 'High': 28.70857048034668,\n",
       " 'Low': 28.117143630981445,\n",
       " 'Close': 28.421428680419922,\n",
       " 'Adj Close': 24.89799690246582,\n",
       " 'Volume': 175933100}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowLast = df.filter(\"Date == '\"+end_date+\"'\").take(1)\n",
    "dictLast = rowLast[0].asDict()\n",
    "\n",
    "dictLast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora per ottenere la variazione in percentuale ci basta applicare la seguente formula:\n",
    "<br><br>\n",
    "$$\\frac{close-open}{close}*100$$\n",
    "e arrotondiamo il risultato a 2 cifre dopo la virgola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.69"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round((dictLast[\"Close\"]-dictFirst[\"Open\"])/dictLast[\"Close\"]*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un +38.7% per la gioia degli investitori :D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
